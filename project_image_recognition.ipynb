{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N2MHhpLovpKK"
      },
      "source": [
        "# Image Recognition model between Cats and Dogs\n",
        "$\\textrm{using Convolutional Neural Network (CNN) [Deep Learning]}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXiCk6ylwXII"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lytlZd9ZvHcx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AqsTZg0C0SbX",
        "outputId": "073f3f6e-7539-461d-ac96-d0ab5ac1b07b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.12.0'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkYPx4qDxyyL"
      },
      "source": [
        "## PART 1: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAi5MbXs0pS6"
      },
      "source": [
        "### Preprocessing Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OJHdjmv60o1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "#Keras Libary APi for Image Preprocessing:\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "      #Divide all pixel values by 255. Hence all values lies b/w 0 to 1\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "#Importing Image from training Set\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        'Dataset/training_set',\n",
        "        target_size=(64,64),#Image Final Dimention which will be feed to model\n",
        "        batch_size=32,#Number of Images in each Batch\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdoyigOb0ueo"
      },
      "source": [
        "### Preprocessing Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ah2ikMOS0zCR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "#Keras Libary APi for Image Preprocessing:\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#Importing Image from test Set\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "    'Dataset/test_set',\n",
        "    target_size = (64, 64),#Image Final Dimention which will be feed to model\n",
        "    batch_size = 32,#Number of Images in each Batch\n",
        "    class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKjmIhlY_lJe"
      },
      "source": [
        "## PART 2: Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4OxIGXo_7If"
      },
      "source": [
        "### Initialising the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8RS3khyA_t1t"
      },
      "outputs": [],
      "source": [
        "cnn=tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLEr-a6BBNQS"
      },
      "source": [
        "### Step 1 - Convolution\n",
        "\n",
        "$\\textrm{Input Image}→\\text{Feature Maps}→\\text{Apply reLU{Rectifier Function}}$\n",
        "\n",
        "Applying **Filters/Feture Dectectors** on Original Image to get **Convolation Layes/Feature Map**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MOeOS1_4CnYW"
      },
      "outputs": [],
      "source": [
        "# Adding Convolutional Layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[64,64,3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YOwC0B5Hb6r"
      },
      "source": [
        "### Step 2 - Pooling\n",
        "$\\textrm{Input Image}→\\text{Feature Maps}→\\text{Apply reLU{Rectifier Function}}→\\text{Pooled Featured Layer}$\n",
        "\n",
        "Applying **Filters/Feture Dectectors** on Original Image to get **Convolation Layes/Feature Map** and from that we extract main Features of image leaving useless data which generate **Pooled Featured Layer** {mostly we use Max-Pooling}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WRFDDNURMKk0"
      },
      "outputs": [],
      "source": [
        "# Adding Pooling Layer\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWS6dBFpS_J1"
      },
      "source": [
        "### Adding Secound Convolutional and Pooling Layer\n",
        "{for better processing of feature and better result from image}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Tyx_wcRbTIC2"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOfAFbySTddJ"
      },
      "source": [
        "### Step 3 - Flattening\n",
        "$\\textrm{Input Image}→\\text{Feature Maps}→\\text{Apply reLU{Rectifier Function}}→\\text{Pooled Featured Layer}→\\text{Flattened}$\n",
        "\n",
        "Applying **Filters/Feture Dectectors** on Original Image to get **Convolation Layes/Feature Map** and from that we extract main Features of image leaving useless data which generate **Pooled Featured Layer** {mostly we use Max-Pooling}.\n",
        "\n",
        "This Pooled Featured Layer is in non-UniDimensional array which can be used in input of main Processing of ANN. Hence, We convert it into 1D array[0,1,2,3...n] for processing of ANN. Hence, This Process is known as **Flattening**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RK-DksjAUtBQ"
      },
      "outputs": [],
      "source": [
        "# Adding Pooling Layer\n",
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZgbmtpmXRjc"
      },
      "source": [
        "### Step 4 - Fully Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ADZTImyJXct1"
      },
      "outputs": [],
      "source": [
        "#Hidden Layer\n",
        "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))\n",
        "\n",
        "#Output layer\n",
        "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "#'activation='softmax' {for classfication output}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWHMj9ZBZ3Mr"
      },
      "source": [
        "## PART 3: Training The CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tGhHomWZ8dK"
      },
      "source": [
        "### Compiling CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6ZTBrhQUaHRB"
      },
      "outputs": [],
      "source": [
        "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMKs5AxiaNjX"
      },
      "source": [
        "### Training CNN Model\n",
        "Training the CNN on Training Set and evaluating it on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SriGDzIBas82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 191s 750ms/step - loss: 0.6865 - accuracy: 0.5469 - val_loss: 0.6766 - val_accuracy: 0.5600\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 117s 465ms/step - loss: 0.6291 - accuracy: 0.6464 - val_loss: 0.5922 - val_accuracy: 0.6830\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 129s 517ms/step - loss: 0.5786 - accuracy: 0.6946 - val_loss: 0.5762 - val_accuracy: 0.7050\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 137s 548ms/step - loss: 0.5324 - accuracy: 0.7339 - val_loss: 0.5314 - val_accuracy: 0.7395\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 55s 218ms/step - loss: 0.5183 - accuracy: 0.7384 - val_loss: 0.4985 - val_accuracy: 0.7570\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 118s 472ms/step - loss: 0.4979 - accuracy: 0.7599 - val_loss: 0.4819 - val_accuracy: 0.7615\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 103s 409ms/step - loss: 0.4877 - accuracy: 0.7667 - val_loss: 0.4942 - val_accuracy: 0.7605\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 62s 249ms/step - loss: 0.4763 - accuracy: 0.7729 - val_loss: 0.4818 - val_accuracy: 0.7710\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 63s 249ms/step - loss: 0.4566 - accuracy: 0.7839 - val_loss: 0.4806 - val_accuracy: 0.7740\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 95s 380ms/step - loss: 0.4502 - accuracy: 0.7884 - val_loss: 0.4855 - val_accuracy: 0.7785\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 67s 267ms/step - loss: 0.4427 - accuracy: 0.7926 - val_loss: 0.4657 - val_accuracy: 0.7715\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 43s 171ms/step - loss: 0.4269 - accuracy: 0.7999 - val_loss: 0.4597 - val_accuracy: 0.7870\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 44s 174ms/step - loss: 0.4186 - accuracy: 0.8060 - val_loss: 0.4626 - val_accuracy: 0.7965\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 43s 171ms/step - loss: 0.4066 - accuracy: 0.8166 - val_loss: 0.4742 - val_accuracy: 0.7910\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 54s 216ms/step - loss: 0.4023 - accuracy: 0.8126 - val_loss: 0.4599 - val_accuracy: 0.7915\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 47s 186ms/step - loss: 0.3975 - accuracy: 0.8124 - val_loss: 0.4643 - val_accuracy: 0.7925\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 51s 204ms/step - loss: 0.3871 - accuracy: 0.8240 - val_loss: 0.4619 - val_accuracy: 0.7900\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 51s 203ms/step - loss: 0.3778 - accuracy: 0.8231 - val_loss: 0.4706 - val_accuracy: 0.7890\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 45s 179ms/step - loss: 0.3651 - accuracy: 0.8351 - val_loss: 0.4881 - val_accuracy: 0.7890\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 45s 178ms/step - loss: 0.3701 - accuracy: 0.8301 - val_loss: 0.4534 - val_accuracy: 0.8065\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 54s 217ms/step - loss: 0.3618 - accuracy: 0.8380 - val_loss: 0.4643 - val_accuracy: 0.7925\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 63s 252ms/step - loss: 0.3506 - accuracy: 0.8380 - val_loss: 0.4617 - val_accuracy: 0.7900\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 0.3419 - accuracy: 0.8501 - val_loss: 0.4600 - val_accuracy: 0.8010\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 47s 189ms/step - loss: 0.3407 - accuracy: 0.8495 - val_loss: 0.5584 - val_accuracy: 0.7560\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 44s 175ms/step - loss: 0.3330 - accuracy: 0.8520 - val_loss: 0.4744 - val_accuracy: 0.7990\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1e207fb7050>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnn.fit(x=training_set,validation_data=test_set,epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gge8c6nVdB7y"
      },
      "source": [
        "## Part 4: Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5JC7-TAHdBe4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.utils import load_img,img_to_array\n",
        "def finder(address_of_img:str)->bool:\n",
        "  test_img=load_img(address_of_img,target_size=(64,64))\n",
        "  test_img=img_to_array(test_img)#as CNN expect 2D array\n",
        "  test_img=np.expand_dims(test_img,axis=0) #add fake dimentions\n",
        "  result=cnn.predict(test_img)\n",
        "  training_set.class_indices\n",
        "  return(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qslE83cbp-fs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 49ms/step\n",
            "Dog\n"
          ]
        }
      ],
      "source": [
        "#dog image\n",
        "if finder('Dataset/prediction/test_1.jpg'):\n",
        "  print('Dog')\n",
        "else:\n",
        "  print('Cat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "sZhqy4NY4D8m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 89ms/step\n",
            "Cat\n"
          ]
        }
      ],
      "source": [
        "#cat image\n",
        "if finder('Dataset/prediction/test_2.jpg'):\n",
        "  print('Dog')\n",
        "else:\n",
        "  print('Cat')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best Regards,\n",
        "\n",
        "Lakshya Sharma"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
